{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/user/env python3\n",
    "'''This script pulls in inventory data, \n",
    "   builds and tests several predictive models performance on the data'''\n",
    "   \n",
    "__author__ = 'Sam M. Mfalila'\n",
    "__email__ = 'sam.mfalila@gmail.com'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline, make_pipeline\n",
    "import sklearn.preprocessing\n",
    "from sklearn.compose import make_column_transformer, ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define data file path and target variable\n",
    "#file = 'derived_data/train_data_abtv4_undersampled.csv'\n",
    "file = 'derived_data/balanced.csv'\n",
    "target = 'went_on_backorder_Yes'\n",
    "seed = 777\n",
    "\n",
    "class Data():\n",
    "    '''Loads data, samples training data if specified, assigns features_df and target_df\n",
    "    '''\n",
    "    def __init__(self, file, target, sample=False, n_samples=None, frac=None):\n",
    "        self.file = file\n",
    "        self. sample = sample\n",
    "        self.n_sample = n_sample\n",
    "        self.target = target\n",
    "        self.frac = frac\n",
    "    \n",
    "    def get_data(file, sample=False, n_samples=None, frac=None):\n",
    "        '''Load train data with option to sample'''\n",
    "        data = pd.read_csv(file)\n",
    "        if sample:\n",
    "            '''Sample train data due to resource limitation'''\n",
    "            #data = data.sample(n_samples, random_state=123)\n",
    "            data = data.groupby(target).apply(lambda x: x.sample(frac=frac))\n",
    "            data = data.reset_index(drop=True)\n",
    "        else:\n",
    "            data = data\n",
    "        print(data.shape, 'data loaded\\n')\n",
    "        return data\n",
    "         \n",
    "    def get_features():\n",
    "        '''Assign features dataframe'''\n",
    "        features_df = data\n",
    "        print(features_df.shape, 'features assigned\\n')\n",
    "        return features_df\n",
    "        \n",
    "    def get_target():\n",
    "        '''Assign target'''\n",
    "        target_df = data[target].values\n",
    "        print(target_df.shape, '...target rows loaded\\n')\n",
    "        return target_df\n",
    "\n",
    "\n",
    "    \n",
    "class FeatureSelector(BaseEstimator, TransformerMixin):\n",
    "    '''Custom transformer to extract columns passed as arguments'''\n",
    "    \n",
    "    def __init__(self, feature_names):\n",
    "        '''Class constructor'''\n",
    "        self._feature_names = feature_names\n",
    "        \n",
    "    def fit(self, features_df, target = None):\n",
    "        '''Returns self and nothing else'''\n",
    "        return self\n",
    "    \n",
    "    def transform( self, features_df, target = None):\n",
    "        '''This method returns selected features'''\n",
    "        return features_df[ self._feature_names]      \n",
    "    \n",
    "\n",
    "\n",
    "class DropMissing(BaseEstimator, TransformerMixin):\n",
    "    '''Takes df, drops all missing\n",
    "    '''\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        \n",
    "    def fit(self, df, target=None):\n",
    "        '''Returns self, nothing else.\n",
    "        '''\n",
    "        return self\n",
    "    \n",
    "    def transform(self, df, target=None):\n",
    "        df.dropna(axis=0, how='any', inplace=True)        \n",
    "        return df\n",
    "    \n",
    "    \n",
    "\n",
    "class CategoricalFeatsAdded(BaseEstimator, TransformerMixin):\n",
    "    ''' A custom transformer to add 'neg_inv_balance' indicator \n",
    "        Takes df, checks if 'national_inv' is negative and adds indicator variable\n",
    "    '''\n",
    "    def __init__ (self, neg_inv_balance=True, low_inventory=True, \\\n",
    "                  low_intransit=True, high_forecast=True):\n",
    "        ''' class constructor'''\n",
    "        self._neg_inv_balance = neg_inv_balance\n",
    "        self._low_inventory = low_inventory\n",
    "        self._low_intransit = low_intransit\n",
    "        self._high_forecast = high_forecast\n",
    "    \n",
    "    def fit( self, features_df, target = None):\n",
    "        ''' returns self, nothing else is done here'''\n",
    "        return self\n",
    "    \n",
    "    def transform(self, features_df, target = None):\n",
    "        ''' creates aformentioned features and drops redundant ones'''\n",
    "\n",
    "        if self._neg_inv_balance:\n",
    "            '''check if needed'''\n",
    "            features_df['neg_inv_balance'] = (features_df.national_inv < 0).astype(int) \n",
    "            \n",
    "        if self._low_inventory:\n",
    "            '''check if needed'''\n",
    "            features_df['low_inventory'] = (data['national_inv'] < \\\n",
    "                                            data['national_inv'].median()).astype(int)\n",
    "            \n",
    "        if self._low_intransit:\n",
    "            '''check if needed'''\n",
    "            features_df['low_intransit'] = (data['in_transit_qty'] < \\\n",
    "                                            data['in_transit_qty'].mean()).astype(int)\n",
    "            \n",
    "        if self._high_forecast:\n",
    "            '''check if needed'''\n",
    "            features_df['high_forcast'] = (data['forecast_3_month'] > \\\n",
    "                                           data['forecast_3_month'].mean()).astype(int)\n",
    "\n",
    "        return features_df.values\n",
    "\n",
    "    \n",
    "    \n",
    "class RemoveNegativeValues(BaseEstimator, TransformerMixin):\n",
    "    '''Takes df, converts all negative values to positive\n",
    "    '''\n",
    "    def __init__(self, features_df):\n",
    "        self.features_df = features_df\n",
    "        \n",
    "    def fit(self, features_df, target=None):\n",
    "        '''Returns self, does nothing else\n",
    "        '''\n",
    "        return self\n",
    "    \n",
    "    def transform(self, features_df, target=None):\n",
    "        '''Takes df, returns absolute values\n",
    "        '''\n",
    "        features_df = features_df.abs()        \n",
    "        return features_df\n",
    "\n",
    "\n",
    "    \n",
    "class SimpleImputerTransformer(BaseEstimator, TransformerMixin):\n",
    "    '''This transformer imputes missing values'''\n",
    "    def __init__(self, features_df, target=None):\n",
    "        self.features_df = features_df\n",
    "        \n",
    "    def fit(self, features_df, target=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, features_df, target=None):\n",
    "        imputer = SimpleImputer(missing_values = np.NaN,\n",
    "                                strategy='mean')\n",
    "        \n",
    "        # Fit data to the imputer object \n",
    "        imputer = imputer.fit(features_df)\n",
    "        \n",
    "        # Impute the data      \n",
    "        imputed = imputer.transform(features_df)\n",
    "        \n",
    "        features_df = pd.DataFrame(data=imputed)\n",
    "    \n",
    "        return features_df    \n",
    "    \n",
    "    \n",
    "    \n",
    "class CapOutliers(BaseEstimator, TransformerMixin):\n",
    "    '''Takes df, caps outliers\n",
    "    '''\n",
    "    def __init__(self, features_df):\n",
    "        self.features_df = features_df\n",
    "        \n",
    "    def fit(self, features_df, Target=None):\n",
    "        '''Returns self, does nothing else\n",
    "        '''\n",
    "        return self\n",
    "    \n",
    "    def transform(self, features_df, q=0.90, target=None):\n",
    "        for col in features_df.columns:\n",
    "\n",
    "            if (((features_df[col].dtype)=='float64') | ((features_df[col].dtype)=='int64')):\n",
    "                percentiles = features_df[col].quantile([0.01,q]).values\n",
    "                features_df[col][features_df[col] <= percentiles[0]] = percentiles[0]\n",
    "                features_df[col][features_df[col] >= percentiles[1]] = percentiles[1]\n",
    "            else:\n",
    "                features_df[col]=features_df[col]\n",
    "        return features_df\n",
    "    \n",
    "    \n",
    "    \n",
    "class StandardScalerTransformer(BaseEstimator, TransformerMixin):\n",
    "    ''' This transformer standardizes all numerical features'''\n",
    "    def __init__(self, features_df, target=None):\n",
    "        self.features_df = features_df\n",
    "        \n",
    "    def fit(self, features_df, target=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, features_df, target=None):\n",
    "        col_names = numerical_features\n",
    "        features = features_df[col_names]\n",
    "        scaler = StandardScaler().fit(features)\n",
    "        features_df = scaler.transform(features)\n",
    "        return features_df     \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "class DelUnusedCols(BaseEstimator, TransformerMixin):\n",
    "    '''This transformer deletes unused columns from a data pipeline\n",
    "       Col 0 holds an extra column for 'national_inv' added through the categorical feats. pipeline.\n",
    "       This row is no longer needed after new categorical features leveraging the column are engineered\n",
    "    '''\n",
    "    def __init__(self, features_df, target=None):\n",
    "        self.features_df = features_df\n",
    "        \n",
    "    def fit(self, features_df, target=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, features_df, target=None):\n",
    "        a = features_df\n",
    "        a = np.delete(a,0,1)\n",
    "        features_df = a\n",
    "        return features_df\n",
    "\n",
    "\n",
    "    \n",
    "class SplitData(object):\n",
    "    '''Takes prepared data, performs train test split\n",
    "    '''\n",
    "    prepared_features_df = pd.DataFrame()\n",
    "    feats = None\n",
    "    target = None\n",
    "    train_feats = None\n",
    "    test_feats = None\n",
    "    train_target = None\n",
    "    test_target = None\n",
    "    \n",
    "    def get_dataframe(prepared_features):\n",
    "        '''Takes prepared features array, returns dataframe'''\n",
    "        SplitData.prepared_features_df = pd.DataFrame(data=prepared_features)\n",
    "    \n",
    "        #get features and target data\n",
    "        SplitData.feats = SplitData.prepared_features_df.drop([0], axis=1)\n",
    "        SplitData.target = SplitData.prepared_features_df[0]\n",
    "    \n",
    "    def split_data(test_frac):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(SplitData.feats, SplitData.target,\\\n",
    "                                                            test_size=test_frac, random_state=seed,\\\n",
    "                                                            stratify=SplitData.target)\n",
    "        #save datasets\n",
    "        SplitData.train_feats = X_train\n",
    "        SplitData.test_feats = X_test\n",
    "        SplitData.train_target = y_train\n",
    "        SplitData.test_target = y_test\n",
    "        \n",
    "        print('\\nTrain and test data assigned\\n')        \n",
    "  \n",
    "    \n",
    "        \n",
    "class Models(object):\n",
    "    '''This class holds all modeling objects\n",
    "       Note: Instantiate any additional models to test as class variables below    \n",
    "    '''\n",
    "    estimators = {}\n",
    "    fitted_grid = {}\n",
    "    param_grids = {}\n",
    "    best_models = {}\n",
    "    best_score = None \n",
    "    best_model = []\n",
    "    best_params = {}\n",
    "    best_model_params = []\n",
    "    best_model_list = []\n",
    "    best_estimator = {}\n",
    "    lr = LogisticRegression(random_state=seed)\n",
    "    sv = SVC(random_state=seed)\n",
    "    ld = LinearDiscriminantAnalysis()\n",
    "    sg = SGDClassifier(random_state=seed)\n",
    "    rf = RandomForestClassifier(random_state=seed)\n",
    "    gb = GradientBoostingClassifier(random_state = seed)\n",
    "    \n",
    "    results_dict={}\n",
    "    \n",
    "    def __init__(self, master, logreg, svc,lda, sgd, randforest, gradboost, \n",
    "                 n_iter, scoring, n_jobs, train_features):\n",
    "        self.master = master\n",
    "        self.logreg = logreg\n",
    "        self.svc = svc\n",
    "        self.lda = lda\n",
    "        self.sgd = sgd\n",
    "        self.randforest= randforest\n",
    "        self.gradboost = gradboost\n",
    "        self.n_iter = n_iter\n",
    "        self.scoring = scoring\n",
    "        self.n_jobs = n_jobs\n",
    "        self.train_features = train_features  \n",
    "        \n",
    "    def hyperparameters(logreg=True, svc=True, lda=True,\n",
    "                        sgd=True, randforest=True, gradboost=True):\n",
    "        '''Define model hyperparameters for tuning\n",
    "           Add aditional models hyperparameters for tuning here as needed        \n",
    "        '''\n",
    "        if logreg:\n",
    "            '''set lr hyperparameters for tuning'''\n",
    "            #select hyperparameters for logreg\n",
    "            lr_solver_options = ['saga']\n",
    "            lr_C_options = [0.001,0.01,0.1,1,10,100]\n",
    "            \n",
    "            #set param grid for lr\n",
    "            lr_param_grid = dict(solver = lr_solver_options,\\\n",
    "                                 C = lr_C_options)\n",
    "            \n",
    "            #add param grid to param_grids\n",
    "            Models.param_grids['lr'] = lr_param_grid\n",
    "\n",
    "        if svc:\n",
    "            '''set svc hyperparameters for tuning'''\n",
    "            #select hyperparameters for svc\n",
    "            sv_kernel_options = ['linear','rbf', 'poly', 'sigmoid']\n",
    "            sv_C_options = [0.001,0.01,0.1,1,10,100]\n",
    "            \n",
    "            #set param grid for svc\n",
    "            sv_param_grid = dict(kernel = sv_kernel_options,\\\n",
    "                                 C = sv_C_options)\n",
    "            \n",
    "            #add param grid to param_grids\n",
    "            Models.param_grids['sv'] = sv_param_grid\n",
    "            \n",
    "        if lda:\n",
    "            '''set lda hyperparameters'''\n",
    "            ld_solver_options = ['svd', 'lsqr', 'eigen']\n",
    "            \n",
    "            #set hyperparameter grid for lda\n",
    "            ld_param_grid = dict(solver = ld_solver_options)\n",
    "            \n",
    "            #add param grid to param_grids\n",
    "            Models.param_grids['ld'] = ld_param_grid\n",
    "            \n",
    "        if sgd:\n",
    "            '''set sgd hyperparameters'''\n",
    "            sg_max_iter_options = ['1000', '100000']\n",
    "            sg_tol_options = [1e-3]\n",
    "            \n",
    "            #set hyperparameter grid for sgd\n",
    "            sg_param_grid = dict(max_iter = sg_max_iter_options,\\\n",
    "                                 tol = sg_tol_options)\n",
    "            \n",
    "            #add param grid to param_grids\n",
    "            Models.param_grids['sg'] = sg_param_grid\n",
    "                                    \n",
    "        if randforest:\n",
    "            '''set rf hyperparameters for tuning'''\n",
    "            #select hyperparameters for rf\n",
    "            rf_n_estimators_options =[10,75,100,150,200,1000]\n",
    "            rf_max_features_options = ['auto','sqrt', 'log2',  0.33]\n",
    "            \n",
    "            #set param grid for rf\n",
    "            rf_param_grid = dict(n_estimators = rf_n_estimators_options,\\\n",
    "                                 max_features = rf_max_features_options)\n",
    "            \n",
    "            #add param grid to param_grids\n",
    "            Models.param_grids['rf'] = rf_param_grid\n",
    "        \n",
    "        if gradboost:\n",
    "            '''set gb hyperparameters for tuning'''\n",
    "            #select hyperparameters for gb\n",
    "            gb_n_estimators_options = [10, 100, 200, 1000]\n",
    "            gb_learning_rate_options = [0.001, 0.01, 0.05, 0.1, 0.2]\n",
    "            gb_max_depth_options = [1, 3, 5, 7, 9]\n",
    "            gb_subsample_options = [0.5,0.7,1.0]\n",
    "            \n",
    "            #set param grid for gb\n",
    "            gb_param_grid = dict(learning_rate = gb_learning_rate_options,\\\n",
    "                                 n_estimators = gb_n_estimators_options,\\\n",
    "                                 subsample = gb_subsample_options,\\\n",
    "                                 max_depth = gb_max_depth_options)\n",
    "            \n",
    "            #add param grid to param_grids\n",
    "            Models.param_grids['gb'] = gb_param_grid\n",
    "               \n",
    "        print('Hyperparameter grid is set\\n')\n",
    "        \n",
    "        \n",
    "    def setting_gridsearch(logreg=False, svc=False, lda=False, sgd=False,\\\n",
    "                           randforest=False, gradboost=False):\n",
    "        '''Selects estimators to tune and holds them in a dictionary\n",
    "           Add any additional models for fiting here as needed        \n",
    "        \n",
    "           Create a class empty dict \"estimators\" to hold estimators for GridSearchCV\n",
    "           Add estimators to estimators dict\n",
    "        '''\n",
    "        if logreg:\n",
    "            Models.estimators['lr'] = Models.lr\n",
    "        if svc:\n",
    "            Models.estimators['sv'] = Models.sv\n",
    "        if lda:\n",
    "            Models.estimators['ld'] = Models.ld\n",
    "        if sgd:\n",
    "            Models.estimators['sg'] = Models.sg\n",
    "        if randforest:\n",
    "            Models.estimators['rf'] = Models.rf       \n",
    "        if gradboost:\n",
    "            Models.estimators['gb'] = Models.gb            \n",
    "            \n",
    "        print('\\nGridSearch object set and ready for fitting\\n')\n",
    "        \n",
    "                       \n",
    "    def check_hyperparams_settings():\n",
    "        '''Running code to check that hyperparameters is set up correctly.\n",
    "        '''\n",
    "        print('Validate gridsearch object set correctly...\\n')\n",
    "        for key in ['lr','sv','ld','sg','rf', 'gb']:\n",
    "            if key in Models.param_grids:\n",
    "                if type(Models.param_grids[key]) is dict:\n",
    "                    print( key, 'was found in hyperparameters, and it is a grid.' )\n",
    "                else:\n",
    "                    print( key, 'was found in hyperparameters, but it is not a grid.' )\n",
    "            else:\n",
    "                print( key, 'was not found in hyperparameters')\n",
    "                \n",
    "                \n",
    "    def fit_models(n_iter, scoring, n_jobs, train_features):\n",
    "        '''Fits all models in GridSearch with k-folds cross validation\n",
    "         Args:\n",
    "           cv - Number of cross validation splits\n",
    "           scoring - Scoring metric\n",
    "           n_jobs - Number of processors to use if parrallel processing available(-1 means using all processors)\n",
    "         \n",
    "         Notes:\n",
    "           Created a class dict 'fitted_grid' to hold each of fitted model\n",
    "           \n",
    "           GridSearch only stores results from cross_val for the last fitted model.\n",
    "           We need to append results of each model fit to the dictionary above so we can access those attributes as needed.\n",
    "        '''\n",
    "        print('\\nModel fitting started...')\n",
    "        for Models.name, Models.estimator in Models.estimators.items():\n",
    "            full_grid = RandomizedSearchCV(Models.estimators[Models.name], \n",
    "                                     Models.param_grids[Models.name], n_iter=n_iter, scoring = scoring,\n",
    "                                     n_jobs = n_jobs)  \n",
    "            \n",
    "            #fit data to GridSearchCV object\n",
    "            full_grid.fit(SplitData.train_feats, SplitData.train_target)    \n",
    "\n",
    "            #store fitted model\n",
    "            Models.fitted_grid[Models.name] = full_grid    \n",
    "            \n",
    "            print(Models.name,'has been fitted,')\n",
    "        \n",
    "        print('\\nModel fitting completed.\\n')\n",
    "        \n",
    "    \n",
    "    def summarize_classification(y_test, y_pred):\n",
    "        '''Takes test data, returns model summaries'''\n",
    "        acc = accuracy_score(y_test, y_pred, normalize=True)\n",
    "        num_acc = accuracy_score(y_test, y_pred, normalize=False)\n",
    "        \n",
    "        prec = precision_score(y_test, y_pred)\n",
    "        recall = recall_score(y_test, y_pred)\n",
    "        \n",
    "        return {'accuracy:': round(acc,4),\n",
    "                'precision:': round(prec, 4),\n",
    "                'recall:':round(recall, 4),\n",
    "                'accuracy_count:':num_acc\n",
    "               }\n",
    "\n",
    "    \n",
    "    def get_predictions():\n",
    "               \n",
    "        for Models.name, Models.model in Models.fitted_grid.items():\n",
    "                        \n",
    "            y_pred_train = Models.model.predict(SplitData.train_feats)\n",
    "            \n",
    "            y_pred_test = Models.model.predict(SplitData.test_feats)\n",
    "            \n",
    "            train_summary = Models.summarize_classification(SplitData.train_target, y_pred_train) #Note: Create class level variables for these\n",
    "            test_summary = Models.summarize_classification(SplitData.test_target, y_pred_test)\n",
    "            \n",
    "            pred_results = pd.DataFrame({'y_test': SplitData.test_target,\n",
    "                                         'y_pred': y_pred_test})\n",
    "            \n",
    "            model_crosstab = pd.crosstab(pred_results.y_pred, pred_results.y_test)\n",
    "            \n",
    "            Models.results_dict[Models.name] = {'training': train_summary,\n",
    "                                  'test': test_summary,\n",
    "                                  'confussion_matrix': model_crosstab\n",
    "                                 }\n",
    "            \n",
    "        print('Predictions done...\\n')\n",
    "        \n",
    "    \n",
    "    def compare_results():\n",
    "        \n",
    "        print('\\n{0:*^80}\\n'.format(' Model Results '))\n",
    "        \n",
    "        for key in Models.results_dict:\n",
    "            print('Classification: ', key)\n",
    "                \n",
    "            print()\n",
    "            print('Training Data')\n",
    "            for score in Models.results_dict[key]['training']:\n",
    "                print(score, Models.results_dict[key]['training'][score])\n",
    "                    \n",
    "            print()\n",
    "            print('Test Data')\n",
    "            for score in Models.results_dict[key]['test']:\n",
    "                    print(score, Models.results_dict[key]['test'][score])\n",
    "                    \n",
    "            print()\n",
    "            print('Confussion Matrix')\n",
    "            print(Models.results_dict[key]['confussion_matrix'])\n",
    "                        \n",
    "            print()\n",
    "            print()\n",
    "            print()\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45267, 9) data loaded\n",
      "\n",
      "(45267, 9) features assigned\n",
      "\n",
      "(45267,) ...target rows loaded\n",
      "\n",
      "\n",
      "Train and test data assigned\n",
      "\n",
      "\n",
      "Train and test data assigned\n",
      "\n",
      "Hyperparameter grid is set\n",
      "\n",
      "Validate gridsearch object set correctly...\n",
      "\n",
      "lr was found in hyperparameters, and it is a grid.\n",
      "sv was not found in hyperparameters\n",
      "ld was found in hyperparameters, and it is a grid.\n",
      "sg was not found in hyperparameters\n",
      "rf was not found in hyperparameters\n",
      "gb was not found in hyperparameters\n",
      "\n",
      "GridSearch object set and ready for fitting\n",
      "\n",
      "\n",
      "Model fitting started...\n",
      "lr has been fitted,\n",
      "ld has been fitted,\n",
      "\n",
      "Model fitting completed.\n",
      "\n",
      "Predictions done...\n",
      "\n",
      "\n",
      "******************************** Model Results *********************************\n",
      "\n",
      "Classification:  lr\n",
      "\n",
      "Training Data\n",
      "accuracy: 0.8068\n",
      "precision: 0.7709\n",
      "recall: 0.5981\n",
      "accuracy_count: 29216\n",
      "\n",
      "Test Data\n",
      "accuracy: 0.8088\n",
      "precision: 0.7765\n",
      "recall: 0.5987\n",
      "accuracy_count: 7323\n",
      "\n",
      "Confussion Matrix\n",
      "y_test   0.0   1.0\n",
      "y_pred            \n",
      "0.0     5516  1211\n",
      "1.0      520  1807\n",
      "\n",
      "\n",
      "\n",
      "Classification:  ld\n",
      "\n",
      "Training Data\n",
      "accuracy: 0.7853\n",
      "precision: 0.7412\n",
      "recall: 0.5468\n",
      "accuracy_count: 28438\n",
      "\n",
      "Test Data\n",
      "accuracy: 0.7889\n",
      "precision: 0.7538\n",
      "recall: 0.5447\n",
      "accuracy_count: 7143\n",
      "\n",
      "Confussion Matrix\n",
      "y_test   0.0   1.0\n",
      "y_pred            \n",
      "0.0     5499  1374\n",
      "1.0      537  1644\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    #load data\n",
    "    data = Data.get_data(file, sample=True, frac=0.1)\n",
    "    \n",
    "    #get Features\n",
    "    features_df = Data.get_features()\n",
    "    \n",
    "    #get target\n",
    "    target_df = Data.get_target()    \n",
    "    \n",
    "    '''Data Transformation Pipelines'''\n",
    "    #categrical features to pass down the categorical pipeline \n",
    "    categorical_features = ['national_inv', 'went_on_backorder_Yes']\n",
    "    \n",
    "    #numerical features to pass down the numerical pipeline \n",
    "    numerical_features = ['national_inv','lead_time','in_transit_qty',\\\n",
    "                          'forecast_3_month']\n",
    "    \n",
    "    #define steps in the categorical pipeline \n",
    "    categorical_pipeline = Pipeline( steps = [ ('cat_selector', FeatureSelector(categorical_features)),\n",
    "                                              \n",
    "                                               ('cat_feats_add', CategoricalFeatsAdded()),\n",
    "                                              \n",
    "                                               ('delete_unused', DelUnusedCols(features_df))\n",
    "                                              \n",
    "                                             ])\n",
    "    \n",
    "    #define the steps in the numerical pipeline \n",
    "    numerical_pipeline = Pipeline( steps = [ ('num_selector', FeatureSelector(numerical_features)),                                        \n",
    "                                       \n",
    "                                             ('remove_negative_values', RemoveNegativeValues(features_df)),\n",
    "                                            \n",
    "                                             ('standard_trans', StandardScalerTransformer(features_df)),\n",
    "                                        \n",
    "                                             ('impute_missing', SimpleImputerTransformer(features_df)),\n",
    "                                        \n",
    "                                             ('cap_outliers', CapOutliers(features_df))                                        \n",
    "                                   \n",
    "                                           ] )    \n",
    "    \n",
    "    #combine numerical and categorical piepline into one full big pipeline horizontally using FeatureUnion\n",
    "    full_pipeline = FeatureUnion(transformer_list = [('categorical_pipeline', categorical_pipeline),\n",
    "                                                     \n",
    "                                                     ('numerical_pipeline', numerical_pipeline)])\n",
    "    \n",
    "    \n",
    "    #disable pandas chained_assignment warning\n",
    "    pd.options.mode.chained_assignment = None\n",
    "    \n",
    "    #fit data to data transformation pipeline\n",
    "    prepared_features = full_pipeline.fit_transform(features_df)\n",
    "    \n",
    "    '''Split Data'''\n",
    "    #get prepared features dataframe\n",
    "    SplitData.get_dataframe(prepared_features)\n",
    "    \n",
    "    #train test split\n",
    "    SplitData.split_data(0.2)   \n",
    "    \n",
    "    '''Data Modeling'''\n",
    "    #Get prepared features dataframe\n",
    "    SplitData.get_dataframe(prepared_features)\n",
    "    \n",
    "    #Spit train and test sets\n",
    "    SplitData.split_data(0.2)\n",
    "    \n",
    "    #Set hyperparameters grid\n",
    "    Models.hyperparameters(logreg=True, svc=False, lda=True, sgd=False,\\\n",
    "                           randforest=False, gradboost=False)\n",
    "    \n",
    "    #Validate hyperparameters set correctly\n",
    "    Models.check_hyperparams_settings()\n",
    "    \n",
    "    #Set models for fitting\n",
    "    Models.setting_gridsearch(logreg=True, svc=False, lda=True, sgd=False,\\\n",
    "                              randforest=False, gradboost=False)\n",
    "    \n",
    "    #Fit models that were set in GridSearch object \n",
    "    Models.fit_models(n_iter=10, scoring= 'average_precision', n_jobs = -1,\\\n",
    "                      train_features = SplitData.prepared_features_df )   \n",
    "    \n",
    "    '''Modeling Results'''\n",
    "    #Get predictions\n",
    "    Models.get_predictions()\n",
    "    \n",
    "    #Get model performance results\n",
    "    Models.compare_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              total        used        free      shared  buff/cache   available\n",
      "Mem:           9041        2410        4480         140        2150        6229\n",
      "Swap:          2047           0        2047\n"
     ]
    }
   ],
   "source": [
    "!free -m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45267, 9) data loaded\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#load data\n",
    "data = Data.get_data(file, sample=True, frac=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45267, 9) features assigned\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#get Features\n",
    "features_df = Data.get_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45267,) ...target rows loaded\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#get target\n",
    "target_df = Data.get_target()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Transforming Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Categrical features to pass down the categorical pipeline \n",
    "categorical_features = ['national_inv', 'went_on_backorder_Yes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Numerical features to pass down the numerical pipeline \n",
    "numerical_features = ['national_inv','lead_time','in_transit_qty',\n",
    "              'forecast_3_month']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining steps in the categorical pipeline \n",
    "categorical_pipeline = Pipeline( steps = [ ('cat_selector', FeatureSelector(categorical_features)),\n",
    "                                                                                                                          \n",
    "                                           ('cat_feats_add', CategoricalFeatsAdded()),\n",
    "                                          \n",
    "                                           ('delete_unused', DelUnusedCols(features_df))\n",
    "                                                                                                                                                                                                            \n",
    "                                         ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the steps in the numerical pipeline \n",
    "numerical_pipeline = Pipeline( steps = [ ('num_selector', FeatureSelector(numerical_features)),                                        \n",
    "                                       \n",
    "                                         ('remove_negative_values', RemoveNegativeValues(features_df)),\n",
    "                                        \n",
    "                                         ('standard_trans', StandardScalerTransformer(features_df)),\n",
    "                                        \n",
    "                                         ('impute_missing', SimpleImputerTransformer(features_df)),\n",
    "                                        \n",
    "                                         ('cap_outliers', CapOutliers(features_df))                                        \n",
    "                                   \n",
    "                                       ] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combining numerical and categorical piepline into one full big pipeline horizontally using FeatureUnion\n",
    "full_pipeline = FeatureUnion(transformer_list = [('categorical_pipeline', categorical_pipeline), \n",
    "                                                  \n",
    "                                                 ('numerical_pipeline', numerical_pipeline)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Disabling pandas chained_assignment warning\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fitting our data to our data transformation pipeline\n",
    "prepared_features = full_pipeline.fit_transform(features_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get prepared features dataframe\n",
    "SplitData.get_dataframe(prepared_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train and test data assigned\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Train test split\n",
    "SplitData.split_data(0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Model Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Hyperparameter Grids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameter grid is set\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Set hyperparameters grid\n",
    "Models.hyperparameters(logreg=True, svc=False, lda=True, sgd=False,\\\n",
    "                       randforest=False, gradboost=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validate gridsearch object set correctly...\n",
      "\n",
      "lr was found in hyperparameters, and it is a grid.\n",
      "sv was not found in hyperparameters\n",
      "ld was found in hyperparameters, and it is a grid.\n",
      "sg was not found in hyperparameters\n",
      "rf was not found in hyperparameters\n",
      "gb was not found in hyperparameters\n"
     ]
    }
   ],
   "source": [
    "#Validate hyperparameters set correctly\n",
    "Models.check_hyperparams_settings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GridSearch object set and ready for fitting\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Set models for fitting\n",
    "Models.setting_gridsearch(logreg=True, svc=False, lda=True, sgd=False,\n",
    "                          randforest=False, gradboost=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              total        used        free      shared  buff/cache   available\n",
      "Mem:           9041        2429        4447         153        2163        6197\n",
      "Swap:          2047           0        2047\n"
     ]
    }
   ],
   "source": [
    "!free -m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model fitting started...\n",
      "lr has been fitted,\n",
      "ld has been fitted,\n",
      "\n",
      "Model fitting completed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Fitting models that were set manually in GridSearchCV \n",
    "Models.fit_models(n_iter=3, scoring= 'average_precision', n_jobs = -1,\n",
    "                  train_features = SplitData.prepared_features_df )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions done...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#get predictions\n",
    "Models.get_predictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "******************************** Model Results *********************************\n",
      "\n",
      "Classification:  lr\n",
      "\n",
      "Training Data\n",
      "accuracy: 0.7973\n",
      "precision: 0.7424\n",
      "recall: 0.6004\n",
      "accuracy_count: 28874\n",
      "\n",
      "Test Data\n",
      "accuracy: 0.7893\n",
      "precision: 0.7297\n",
      "recall: 0.5842\n",
      "accuracy_count: 7146\n",
      "\n",
      "Confussion Matrix\n",
      "y_test   0.0   1.0\n",
      "y_pred            \n",
      "0.0     5383  1255\n",
      "1.0      653  1763\n",
      "\n",
      "\n",
      "\n",
      "Classification:  ld\n",
      "\n",
      "Training Data\n",
      "accuracy: 0.7832\n",
      "precision: 0.733\n",
      "recall: 0.5499\n",
      "accuracy_count: 28362\n",
      "\n",
      "Test Data\n",
      "accuracy: 0.775\n",
      "precision: 0.7162\n",
      "recall: 0.5384\n",
      "accuracy_count: 7017\n",
      "\n",
      "Confussion Matrix\n",
      "y_test   0.0   1.0\n",
      "y_pred            \n",
      "0.0     5392  1393\n",
      "1.0      644  1625\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#get model performance results\n",
    "Models.compare_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Analysis Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our results show that the Support Vectors Classifier (svc) model had the best accuracy count on the test set (3077) without showing indications of overfitting.\n",
    "\n",
    "While the Random Forest(rf) and Gradient Boosting (gb) classifiers had higher accuracy counts, these models appear to be overfitting the training data because their training accuracies are higher than thier test set accuracies.\n",
    "\n",
    "However, all the above three models are resource  greedy and therefore <b/>we'll forcus on training the simplar logistic regression and linear discriminant analysis (lda) models</b> which have, too, provided descent results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
